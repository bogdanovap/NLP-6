{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Часть 3. Классификация текстов\n",
    "Сформулируем для простоты задачу бинарной классификации: будем классифицировать на два класса, то есть, различать резко отрицательные отзывы (с оценкой 1) и положительные отзывы (с оценкой 5).\n",
    "\n",
    "1. Составьте обучающее и тестовое множество: выберите из всего набора данных N1 отзывов с оценкой 1 и N2 отзывов с оценкой 5 (значение N1 и N2 – на ваше усмотрение). Используйте sklearn.model_selection.train_test_split для разделения множества отобранных документов на обучающее и тестовое.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import bz2\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%pylab inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201030it [01:01, 3245.80it/s]\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "with bz2.BZ2File('banki_responses.json.bz2', 'r') as thefile:\n",
    "    for row in tqdm(thefile):\n",
    "        resp = json.loads(row)\n",
    "        if not resp['rating_not_checked'] and (len(resp['text'].split()) > 0):\n",
    "            responses.append(resp)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "df_source = pd.DataFrame(responses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "number_of_cases = 500\n",
    "df_good = df_source[df_source.rating_grade==5][:number_of_cases]\n",
    "df_bad = df_source[df_source.rating_grade==1][:number_of_cases]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "df = pd.concat([df_good, df_bad])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Прежде чем разделять массив на тренировочную и тестовую выборки - проведем подготовку текста"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops  = stopwords.words('russian') + list(punctuation) + [\"``\", \"''\"] + [\n",
    "    'это', 'наш' , 'тыс', 'млн', 'млрд', 'также',  'т', 'д',\n",
    "    'который','прошлый','сей', 'свой', 'наш', 'мочь', 'такой'\n",
    "]\n",
    "ru_words = re.compile(\"[А-Яа-я]+\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from pymorphy3 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def words_only(text):\n",
    "    return \" \".join(ru_words.findall(text))\n",
    "\n",
    "# def lemmatize(text, mystem=m):\n",
    "#     try:\n",
    "#         return \"\".join(m.lemmatize(text)).strip()\n",
    "#     except:\n",
    "#         return \" \"\n",
    "\n",
    "def lemmatize(text, mystem=morph):\n",
    "    try:\n",
    "        return \" \".join([mystem.parse(w)[0].normal_form for w in text.split(' ')]).strip()\n",
    "    except:\n",
    "        return \" \"\n",
    "\n",
    "def remove_stopwords(text, mystopwords = stops):\n",
    "    try:\n",
    "        return \" \".join([token for token in text.split() if not token in mystopwords])\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def preprocess(text):\n",
    "    return remove_stopwords(lemmatize(words_only(text.lower())))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "df.text = df.text.apply(preprocess)\n",
    "df.rating_grade = df.rating_grade.apply(lambda x: 1 if x ==5 else 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Разделяем выборку на тренировочную и тестову.\n",
    "В качестве X - обработанный текст, в качестве y - проставленная оценка (1 или 5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.text, df.rating_grade)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Используйте любой известный вам алгоритм классификации текстов для решения задачи и получите baseline. Сравните разные варианты векторизации текста: использование только униграм, пар или троек слов или с использованием символьных $n$-грам."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89       129\n",
      "           1       0.88      0.90      0.89       121\n",
      "\n",
      "    accuracy                           0.89       250\n",
      "   macro avg       0.89      0.89      0.89       250\n",
      "weighted avg       0.89      0.89      0.89       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89       129\n",
      "           1       0.88      0.90      0.89       121\n",
      "\n",
      "    accuracy                           0.89       250\n",
      "   macro avg       0.89      0.89      0.89       250\n",
      "weighted avg       0.89      0.89      0.89       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# baseline, использование униграм из слов\n",
    "pipe_baseline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1, 1))),\n",
    "    ('clf', LogisticRegression(random_state=42, solver='liblinear'))\n",
    "])\n",
    "pipe_baseline.fit(x_train, y_train)\n",
    "pred = pipe_baseline.predict(x_test)\n",
    "print(classification_report(pred, y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       123\n",
      "           1       0.83      0.81      0.82       127\n",
      "\n",
      "    accuracy                           0.82       250\n",
      "   macro avg       0.82      0.82      0.82       250\n",
      "weighted avg       0.82      0.82      0.82       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# использование 2-грам из слов\n",
    "pipe_bigram = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(2, 2))),\n",
    "    ('clf', LogisticRegression(random_state=42, solver='liblinear'))\n",
    "])\n",
    "pipe_bigram.fit(x_train, y_train)\n",
    "pred = pipe_bigram.predict(x_test)\n",
    "print(classification_report(pred, y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.76      0.73       116\n",
      "           1       0.77      0.72      0.74       134\n",
      "\n",
      "    accuracy                           0.74       250\n",
      "   macro avg       0.74      0.74      0.74       250\n",
      "weighted avg       0.74      0.74      0.74       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# использование 3-грам из слов\n",
    "pipe_trigram = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(3, 3))),\n",
    "    ('clf', LogisticRegression(random_state=42, solver='liblinear'))\n",
    "])\n",
    "pipe_trigram.fit(x_train, y_train)\n",
    "pred = pipe_trigram.predict(x_test)\n",
    "print(classification_report(pred, y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91       128\n",
      "           1       0.90      0.92      0.91       122\n",
      "\n",
      "    accuracy                           0.91       250\n",
      "   macro avg       0.91      0.91      0.91       250\n",
      "weighted avg       0.91      0.91      0.91       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# использование символьных n-gram\n",
    "pipe_char = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(3, 5), analyzer=\"char\")),\n",
    "    ('clf', LogisticRegression(random_state=42, solver='liblinear'))\n",
    "])\n",
    "pipe_char.fit(x_train, y_train)\n",
    "pred = pipe_char.predict(x_test)\n",
    "print(classification_report(pred, y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**ВЫВОД**\n",
    "Самый интересный результат показало использование символьных n-gramm в диапазоне от 3 до 5 символов.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Сравните, как изменяется качество решения задачи при использовании скрытых тем в качестве признаков:\n",
    "- 1-ый вариант: $tf-idf$ преобразование (sklearn.feature_extraction.text.TfidfTransformer) и сингулярное разложение (оно же – латентый семантический анализ) (sklearn.decomposition.TruncatedSVD),\n",
    "- 2-ой вариант: тематические модели LDA (sklearn.decomposition.LatentDirichletAllocation)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89       130\n",
      "           1       0.87      0.90      0.89       120\n",
      "\n",
      "    accuracy                           0.89       250\n",
      "   macro avg       0.89      0.89      0.89       250\n",
      "weighted avg       0.89      0.89      0.89       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "pipe_tsvd = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(3, 5), analyzer=\"char\")),\n",
    "    ('t-svd', TruncatedSVD(n_components=30, random_state=42)),\n",
    "    ('clf', LogisticRegression(random_state=42, solver='liblinear'))\n",
    "])\n",
    "pipe_tsvd.fit(x_train, y_train)\n",
    "pred = pipe_tsvd.predict(x_test)\n",
    "print(classification_report(pred, y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.67      0.71       140\n",
      "           1       0.63      0.71      0.67       110\n",
      "\n",
      "    accuracy                           0.69       250\n",
      "   macro avg       0.69      0.69      0.69       250\n",
      "weighted avg       0.69      0.69      0.69       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "pipe_lda = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(3, 5), analyzer=\"char\")),\n",
    "    ('t-svd', LatentDirichletAllocation(n_components=10, random_state=42, n_jobs=16)),\n",
    "    ('clf', LogisticRegression(random_state=42))\n",
    "])\n",
    "pipe_lda.fit(x_train, y_train)\n",
    "pred = pipe_lda.predict(x_test)\n",
    "print(classification_report(pred, y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**ВЫВОД**\n",
    "Непродолжительные эксперименты с применением способов снижения размерности, не дали прироста в точности.\n",
    "Вероятно, это связано с небольшим количеством данных на входе. Также важно произвести тюнинг гиперпараметров.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
